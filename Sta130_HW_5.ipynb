{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76efd5e8",
   "metadata": {},
   "source": [
    "# 1. \n",
    "\n",
    "### The \"first pre-lecture video\" (above) describes hypothesis testing as addressing \"an idea that can be tested\", and the end of the video then discusses what our actual intended purpose in setting up a null hypothesis is. What is the key factor that makes the difference between ideas that can, and cannot be examined and tested statistically? What would you describe is the key \"criteria\" defining what a good null hypothesis is? And what is the difference between a null hypothesis and an alternative hypothesis in the context of hypothesis testing? Answer these questions with concise explanations in your own words.\n",
    "\n",
    "A key factor that makes an idea statistically testable is whether it can be framed in measurable terms‚Äîusing data\n",
    "to support or refute it. If an idea can‚Äôt be expressed in a way that allows for numerical or observable evaluation, it can‚Äôt be tested statistically.\n",
    "\n",
    "A good null hypothesis (H‚ÇÄ) should be a clear, specific, and falsifiable statement that assumes no effect or difference exists. It serves as the baseline assumption that will either be rejected or not rejected based on the evidence.\n",
    "\n",
    "The null hypothesis (H‚ÇÄ) asserts no change or no effect, while the alternative hypothesis (H‚ÇÅ) proposes the presence of an effect or difference. Hypothesis testing is used to decide whether there‚Äôs enough evidence to reject the null in favor of the alternative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8227d02",
   "metadata": {},
   "source": [
    "# 2.\n",
    "\n",
    "### Towards the end of the 'first pre-lecture' video (above), it is stated that, 'It is important to note that outcomes of tests refer to the population parameter, rather than the sample statistic! As such, the result that we get is for the population.' In terms of the distinctions between the concepts of ùë•ùëñ, ùë• (x bar), ùúá and Œº0 how would you describe what the sentence above means? Explain this concisely in your own words for a 'non-statistical' audience, defining the technical statistical terminology you use in your answer.\n",
    "\n",
    "When we collect data from a sample (a smaller group), we calculate a number like the average (called the sample mean, ùë•) for that sample. However, we are usually interested in knowing something about the entire population (the larger group from which the sample comes), and the population has its own true average (called the population mean, \n",
    "ùúá). When we do hypothesis testing, we use the sample to make an educated guess about the population, but it‚Äôs important to remember that our results are meant to apply to the population as a whole, not just the sample we collected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5791d09",
   "metadata": {},
   "source": [
    "# 3.\n",
    "\n",
    "###  The second \"Pre-lecture\" video (above) explains that we \"imagine a world where the null hypothesis is true\" when calculating a p-value? Explain why this is in your own words in a way that makes the most sense to you.\n",
    "\n",
    "When calculating a p-value, we assume the null hypothesis is true to create a baseline for comparison. This allows us to determine how likely it is to get a result as extreme as the one we observed by chance alone. By imagining the null hypothesis as true, we can measure how surprising the data is under that assumption. If the p-value is very small, it means the observed result is unlikely under the null hypothesis, giving us a reason to doubt or reject it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa877a",
   "metadata": {},
   "source": [
    "# 4.\n",
    "\n",
    "###  The second \"Pre-lecture\" video (above) suggests that a smaller p-value makes the null hypothesis look more ridiculous. Explain why this is in your own words in a way that makes the most sense to you, clarifying the meaning of any technical statistical terminology you use in your answer.\n",
    "\n",
    "A smaller p-value makes the null hypothesis look more \"ridiculous\" because it indicates that the observed data is very unlikely under the assumption that the null hypothesis is true. The p-value measures the probability of obtaining results at least as extreme as those observed, purely by chance, if the null hypothesis is correct. So, when this probability is very small (a low p-value), it suggests that the data doesn't fit well with the null hypothesis, making it seem less believable. In simple terms, the smaller the p-value, the stronger the evidence against the null hypothesis, making it appear increasingly unreasonable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f1974",
   "metadata": {},
   "source": [
    "# 5.\n",
    "\n",
    "### G√ºnt√ºrk√ºn (2003) recorded how kissing couples tilt their heads. 80 out of 124 couples, or 64.5% tilted their heads to the right. Simulate a p-value using a \"50/50 coin-flipping\" model for the assumption of the null hypothesis H0 that the population of humans don't have left or right head tilt tendencies when kissing, and use the table below to determine the level of evidence we have against H0\n",
    " \n",
    "The code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa5f14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n_couples = 124\n",
    "right_tilt_count = 80\n",
    "n_simulations = 10000\n",
    "\n",
    "# Simulate 10,000 \"coin flips\" where each couple has a 50/50 chance of tilting right\n",
    "simulated_tilts = np.random.binomial(n=n_couples, p=0.5, size=n_simulations)\n",
    "\n",
    "# Calculate the p-value: proportion of simulations where the number of right tilts is greater than or equal to 80\n",
    "p_value = np.mean(simulated_tilts >= right_tilt_count)\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24c022",
   "metadata": {},
   "source": [
    "The simulated p-value is 0.0008, which means that there is a very low probability (0.08%) of observing 80 or more couples tilting their heads to the right purely by chance if the null hypothesis is true (50/50 chance).\n",
    "\n",
    "Using the provided table, a p-value of 0.0008 falls in the range of 0.001 ‚â• p, which corresponds to very strong evidence against the null hypothesis. This suggests that it is highly unlikely that the head-tilting behavior is purely random, providing strong evidence that there might be a preference for tilting to the right\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995470d",
   "metadata": {},
   "source": [
    "## Pre-lecture Chatbot Session Summary:\n",
    " \n",
    "#### Chatgpt Session Link: https://chatgpt.com/share/670e137f-c078-8004-a6f4-3d07b3c8a5a7\n",
    "\n",
    "In this session, we discussed several key statistical concepts related to hypothesis testing. We started by exploring the criteria that define a good null hypothesis, emphasizing clarity, testability, and neutrality. Then, we clarified the difference between the null and alternative hypotheses, noting that the null hypothesis assumes no effect while the alternative represents the researcher's proposed effect or difference.\n",
    "\n",
    "We also delved into why it's important to imagine a world where the null hypothesis is true when calculating a p-value, explaining how this assumption helps assess the likelihood of observed data. Finally, we implemented a simple Python simulation of p-value calculation using a 50/50 coin-flipping model, which illustrated how to simulate the distribution of outcomes under the null hypothesis and calculate a p-value based on those results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcaa7a6",
   "metadata": {},
   "source": [
    "# 6.\n",
    "\n",
    "### Can a smaller p-value definitively prove that the null hypothesis is false? Is it possible to definitively prove that Fido (from the \"second pre-lecture video\") is innocent using a p-value? Is it possible to difinitively prove that Fido is guilty using a p-value? How low or high does a p-value have to be to definitely prove one or the other? Explain this concisely in your own words.\n",
    "\n",
    "A smaller p-value cannot definitively prove that the null hypothesis is false, nor can it definitively prove innocence or guilt. P-values indicate the strength of evidence against the null hypothesis, but they do not offer absolute proof. If the p-value is very low (typically below 0.05), it suggests that the observed data is unlikely under the null hypothesis, leading to a rejection of the null. However, this doesn‚Äôt guarantee its falsehood‚Äîit only suggests it‚Äôs unlikely.\n",
    "\n",
    "In the context of Fido, a p-value can suggest strong evidence against innocence or guilt but cannot prove either definitively. P-values are about probability, not certainty, and there is no specific threshold at which we can definitively prove one hypothesis over the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957fb21",
   "metadata": {},
   "source": [
    "# 7.\n",
    "\n",
    "### In the second half of the \"first pre-lecture video\" the concept of a \"one sided\" (or \"one tailed\") test is introduced in contrast to a \"two sided\" (or \"two tailed\") test. Work with a ChatBot to adjust the code from \"Demo II of the Week 5 TUT\" (which revisits the \"Vaccine Data Analysis Assignment\" from Week 04 HW \"Question 8\") in order to compute a p-value for a \"one sided\" (or \"one tailed\") hypothesis test rather than the \"two sided\" (or \"two tailed\") version it provides. Describe (perhaps with the help of your ChatBot) what changed in the code; how this changes the interpretation of the hypothesis test; and whether or not we should indeed expect the p-value to be smaller in the \"one tailed\" versus \"two tailed\" analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e236c5d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'HealthScoreChange'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/4276179057.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Observed statistic: proportion of patients with an increase in HealthScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mobserved_statistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpatient_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHealthScoreChange\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Simulated statistics under the null hypothesis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msimulated_statistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIncreaseProportionSimulations_underH0random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'HealthScoreChange'"
     ]
    }
   ],
   "source": [
    "# Observed statistic: proportion of patients with an increase in HealthScore\n",
    "observed_statistic = (patient_data.HealthScoreChange > 0).mean()\n",
    "\n",
    "# Simulated statistics under the null hypothesis\n",
    "simulated_statistics = IncreaseProportionSimulations_underH0random\n",
    "\n",
    "# One-tailed test (greater than): find simulated statistics greater than or equal to the observed\n",
    "SimStats_as_or_more_extreme_than_ObsStat = simulated_statistics >= observed_statistic\n",
    "\n",
    "# Calculate the p-value for a one-sided test\n",
    "p_value = SimStats_as_or_more_extreme_than_ObsStat.sum() / number_of_simulations\n",
    "\n",
    "# Print results\n",
    "print(\"Number of Simulations: \", number_of_simulations, \"\\n\\n\",\n",
    "      \"Number of simulated statistics (under HO)\\n\",\n",
    "      'that are \"as or more extreme\" than the observed statistic: ',\n",
    "      SimStats_as_or_more_extreme_than_ObsStat.sum(), \"\\n\\n\",\n",
    "      'p-value\\n(= simulations \"as or more extreme\" / total simulations): ', p_value, sep=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd3f6a",
   "metadata": {},
   "source": [
    "  The change in the code modifies the hypothesis test from a two-tailed to a one-tailed test, which affects both the comparison of simulated statistics and the interpretation of the results. In a two-tailed test, the code checks for simulated statistics that are \"as or more extreme\" in both directions (either greater than or less than the observed statistic), meaning it considers deviations on both sides of the null hypothesis value. In contrast, the one-tailed test only checks whether the simulated statistics are in one specific direction‚Äîeither greater than or less than the observed statistic, depending on the hypothesis. This narrower focus leads to a different interpretation: the one-tailed test specifically assesses whether the observed effect is in the direction hypothesized (e.g., greater health improvement), rather than simply whether there is any difference. Since the one-tailed test restricts the \"extreme\" values to just one direction, the p-value in a one-tailed test is generally smaller than in a two-tailed test (assuming the observed statistic is in the expected direction), because fewer simulations are considered as \"extreme\" under the null hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715dd26",
   "metadata": {},
   "source": [
    "# 8.\n",
    "\n",
    "## Problem Introduction\n",
    "\n",
    "This experiment is an adaptation of the famous Fisher‚Äôs Tea Experiment, in which statistician Ronald Fisher tested whether an individual could distinguish whether tea or milk was poured first into a cup. In this adaptation, students from STA130 participated in a similar experiment to determine if they can correctly identify which was poured first: tea or milk. Specifically, out of a random sample of 80 students, 49 students correctly identified which was poured first. This analysis aims to determine whether the students‚Äô performance is due to chance or if they possess a real ability to distinguish between the two pour orders.\n",
    "\n",
    "### Relationship between this experiment and the original with Fisher and Bristol:\n",
    "\n",
    "The original experiment involved a controlled setting where Dr. Muriel Bristol claimed to distinguish whether milk or tea was poured first. In the STA130 version, students are similarly tasked with identifying the pour order, but the sample size is different, and the context is generalized for a classroom setting. While the core hypothesis remains similar‚Äîwhether correct identifications are due to chance‚Äîthe nature of the population and the test is more abstract in the classroom version.\n",
    "\n",
    "### Statements of the Null and Alternative Hypotheses:\n",
    "\n",
    "Null Hypothesis (ùêª0): The students are guessing randomly, with no ability to distinguish between tea and milk pour order. The proportion of correct guesses is ùëù0 = 0.5\n",
    "\n",
    "Alternative Hypothesis (ùêªùê¥): The students can distinguish between tea poured first and milk poured first, leading to a proportion of correct identifications greater than p0 = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34893f54",
   "metadata": {},
   "source": [
    "\n",
    "## Quantitative Analysis\n",
    "\n",
    "### Methodology Code and Explanations\n",
    "We conducted a one-tailed Z-test to determine if the observed proportion of correct identifications (49 out of 80, or 61.25%) was significantly greater than the null hypothesis value of 50%.\n",
    "\n",
    "The Z-score, which measures how many standard deviations the observed result is from the null hypothesis, was found to be 2.01. A Z-score of this magnitude corresponds to a one-tailed p-value of approximately 0.0221. This p-value represents the probability of observing a result as extreme as ours, assuming that students are purely guessing.\n",
    "\n",
    "Since the p-value is less than 0.05 (our typical threshold for significance), we reject the null hypothesis. This indicates that students likely possess some ability to correctly identify whether the milk or tea was poured first.\n",
    "\n",
    "### Findings\n",
    "The one-tailed p-value of 0.0221 suggests that the probability of observing this result, assuming the null hypothesis is true, is just over 2%. Given this result, we reject the null hypothesis at the 5% significance level. This means that there is strong evidence to suggest that students are not simply guessing but may possess some ability to identify whether the milk or tea was poured first.\n",
    "\n",
    "In addition to hypothesis testing, we calculated a 95% confidence interval for the proportion of correct identifications. The interval ranges from approximately 50.3% to 72.2%. Since this interval does not include 50%, it further supports the conclusion that students‚Äô ability to correctly identify the pour order is significantly better than chance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28aa12f",
   "metadata": {},
   "source": [
    "## Conclusion regarding the Null Hypothesis\n",
    "\n",
    "Based on the p-value of 0.0221 and the confidence interval that does not include 50%, we reject the null hypothesis that students are randomly guessing. The results indicate that students have a statistically significant ability to correctly identify whether the tea or milk was poured first. The analysis suggests that their performance exceeds what would be expected by chance alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c117d3d",
   "metadata": {},
   "source": [
    "# 9.\n",
    "\n",
    "Yes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2032f7",
   "metadata": {},
   "source": [
    "## Post-lecture Chatbot Session Summary:\n",
    " \n",
    "#### Chatgpt Session Link: https://chatgpt.com/share/670ef4fa-c4ac-8004-aa80-15a9beeee493\n",
    "\n",
    "In this session, we discussed concepts related to hypothesis testing and statistical significance. You started by asking if a smaller p-value can definitively prove that the null hypothesis is false. I explained that a small p-value does not provide absolute proof; it only suggests strong evidence against the null hypothesis. We covered concepts like statistical significance, practical significance, Type I errors, and the limitations of p-values, emphasizing that statistical results are probabilistic, not definitive.\n",
    "\n",
    "Next, we moved on to a coding question about modifying a Python script to compute a p-value for a one-sided hypothesis test instead of a two-sided one. The original code calculated the p-value by checking if the simulated statistics were \"as or more extreme\" than the observed statistic in both directions, which is the approach for a two-tailed test. I explained the necessary code modifications to perform a one-tailed test, which involves checking only in the relevant direction (either greater than or less than) to match the one-sided hypothesis.\n",
    "\n",
    "I provided the adjusted code for a one-tailed test, along with an explanation of the logic behind it. Specifically, for a one-tailed test, we only look at whether simulated values are greater than (or less than) the observed value, depending on the hypothesis. This approach leads to a narrower range of \"extreme\" values, making the resulting p-value smaller compared to a two-tailed test, assuming the effect is in the hypothesized direction.\n",
    "\n",
    "Finally, we discussed how the change from a two-tailed to a one-tailed test alters the interpretation: the one-tailed test is more focused on whether the observed effect is in the hypothesized direction, rather than testing for any difference in either direction. We also noted that, because fewer simulations are considered \"extreme\" under the one-tailed test, the p-value is typically smaller in a one-tailed analysis compared to a two-tailed one, making it easier to achieve statistical significance if the observed effect matches the direction of the hypothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
